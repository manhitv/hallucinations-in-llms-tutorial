{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **<center>Notebook: Memory for Surprise-driven Exploration (Classic Control Ver.)</center>**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:**\n",
    "1. Burda, Y., Edwards, H., Storkey, A., & Klimov, O. (2018). Exploration by random network distillation. arXiv preprint arXiv:1810.12894.\n",
    "2. Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017, July). Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning (pp. 2778-2787). PMLR.\n",
    "3. Huang, S., Dossa, R. F. J., Ye, C., Braga, J., Chakraborty, D., Mehta, K., & AraÃƒÅ¡jo, J. G. (2022). Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research, 23(274), 1-18.\n",
    "4. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., & Zaremba, W. (2016). Openai gym. arXiv preprint arXiv:1606.01540.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Setting up the libraries** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run these commands from the terminal to install related libraries and set up the working environment\n",
    "# pip install gym # Install the gym library with RL environments\n",
    "# pip install envpool\n",
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, time\n",
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import pandas as pd\n",
    "# import gym\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import envpool\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gym.wrappers.normalize import RunningMeanStd\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. CartPole Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this notebook we will use the simple CartPole-v1 environment (https://www.gymlibrary.dev/environments/classic_control/cart_pole/) for demonstration. ðŸŽ® \n",
    "- The goal in this environment is to balance the pole on top of a cart by applying left or right action.\n",
    "- The observation space is (4,), including cart position, cart velocity, pole angle and pole angular velocity. And, action space is Discrete(2) including action to move left or right. ðŸ”„\n",
    "- This notebook can also be used for other classic control environments as well. ðŸ˜Š All you need to do is change the env_id and the state and action space. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://www.gymlibrary.dev/_images/cart_pole.gif\" width=\"300\" height=\"200\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Simple PPO** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you only want to run the surprise motivation, please run until section 3.2.2 to initialize the parameters and PPO.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'env_id': 'CartPole-v1',                              # env_id can be changed here\n",
    "          'exp_name': \"RND\",\n",
    "          'torch_deterministic': True,\n",
    "          'cuda': True,\n",
    "          'seed': 1,\n",
    "          'num_envs': 8,                                        # number of multi-environments\n",
    "          'num_steps': 128,                                     # number of steps running in each environments per rollout\n",
    "          'num_minibatches': 4,                                 # number of minibatches\n",
    "          'total_timesteps': 100000,                            # total training timesteps\n",
    "          'learning_rate': 2.5e-4,                              # learning_rate\n",
    "          'anneal_lr': True,                                    # reducing learning rate during learning\n",
    "          'num_iterations_obs_norm_init': 50,\n",
    "          'gamma': 0.99,\n",
    "          'int_gamma': 0.99,\n",
    "          'gae_lambda': 0.95,\n",
    "          'int_coef': 1.0,\n",
    "          'ext_coef': 2.0,\n",
    "          'update_epochs': 4,\n",
    "          'update_proportion': 0.25,\n",
    "          'clip_coef': 0.2,\n",
    "          'norm_adv': True,\n",
    "          'clip_vloss': True,\n",
    "          'ent_coef': 0.00,\n",
    "          'vf_coef': 0.5,\n",
    "          'max_grad_norm': 0.5,\n",
    "          'target_kl': None}\n",
    "\n",
    "\n",
    "state_space = 4                                                    # state space of env\n",
    "action_space = 2                                                   # action space of env\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and params[\"cuda\"] else \"cpu\")\n",
    "\n",
    "# Set seed.\n",
    "random.seed(params[\"seed\"])                                                 \n",
    "np.random.seed(params[\"seed\"])\n",
    "torch.manual_seed(params[\"seed\"])\n",
    "torch.backends.cudnn.deterministic = params[\"torch_deterministic\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.1 Utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordEpisodeStatistics(gym.Wrapper):\n",
    "    def __init__(self, env, deque_size=100):\n",
    "        super().__init__(env)\n",
    "        self.num_envs = getattr(env, \"num_envs\", 1)\n",
    "        self.episode_returns = None\n",
    "        self.episode_lengths = None\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observations = super().reset(**kwargs)\n",
    "        self.episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n",
    "        self.episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        self.lives = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        self.returned_episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n",
    "        self.returned_episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n",
    "        return observations\n",
    "\n",
    "    def step(self, action):\n",
    "        observations, rewards, dones, _, infos = super().step(action)\n",
    "        self.episode_returns += infos[\"reward\"]\n",
    "        self.episode_lengths += 1\n",
    "        self.returned_episode_returns[:] = self.episode_returns\n",
    "        self.returned_episode_lengths[:] = self.episode_lengths\n",
    "        self.episode_returns *= 1 - infos[\"terminated\"]\n",
    "        self.episode_lengths *= 1 - infos[\"terminated\"]\n",
    "        infos[\"r\"] = self.returned_episode_returns\n",
    "        infos[\"l\"] = self.returned_episode_lengths\n",
    "        return (observations, rewards, dones, infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)                # Initialize layer weights according to orthogonal method.\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)             # Set the bias of the layer.\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env_id, agent, use_int_rews):\n",
    "    env = gym.make(env_id)\n",
    "    sum_result = 0\n",
    "    for _ in range(50):\n",
    "        ob, _ = env.reset()\n",
    "        while True:\n",
    "            if use_int_rews:\n",
    "                action, _, _, _, _ = agent.get_action_and_value(torch.Tensor(ob).to(device))\n",
    "            else:\n",
    "                action, _, _, _ = agent.get_action_and_value(torch.Tensor(ob).to(device))\n",
    "            next_ob, reward, terminated, truncated, info = env.step(action.cpu().numpy())\n",
    "            sum_result += reward\n",
    "            done = np.logical_or(terminated, truncated)\n",
    "            if done:\n",
    "                break\n",
    "            ob = next_ob\n",
    "    return sum_result/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.2 PPO Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, envs, use_int_rews=False):\n",
    "        super().__init__()\n",
    "        self.use_int_rews = use_int_rews\n",
    "        # print(envs.single_observation_space.shape)\n",
    "\n",
    "        self.critic_ext = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_space, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.critic_int = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_space, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_space, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, action_space), std=0.01),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        if self.use_int_rews:                                                                                       # If intrinsic reward is used\n",
    "            return (action, probs.log_prob(action), probs.entropy(), self.critic_ext(x), self.critic_int(x),)\n",
    "        else:                                                                                                       # If intrinsic reward is not used\n",
    "            return (action, probs.log_prob(action), probs.entropy(), self.critic_ext(x),)\n",
    "\n",
    "    def get_value(self, x):\n",
    "        if self.use_int_rews:                                                                                       # If intrinsic reward is used\n",
    "            return self.critic_ext(x), self.critic_int(x)\n",
    "        else:                                                                                                       # If intrinsic reward is not used\n",
    "            return self.critic_ext(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.3 Main Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = params[\"env_id\"]\n",
    "exp_name = params[\"exp_name\"]\n",
    "seed = params[\"seed\"]\n",
    "run_name = f\"{env_id}__{exp_name}__{seed}__{int(time.time())}\"\n",
    "\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id) for i in range(params[\"num_envs\"])],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up agent and model\n",
    "Agent = PPOAgent(envs, use_int_rews=False).to(device)\n",
    "optimizer = optim.Adam(\n",
    "    Agent.parameters(),\n",
    "    lr=params[\"learning_rate\"],\n",
    "    eps=1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_observation_space.shape).to(device)  \n",
    "actions = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_action_space.shape).to(device)   \n",
    "logprobs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "dones = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "avg_returns = deque(maxlen=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(params[\"num_envs\"] * params[\"num_steps\"])                \n",
    "minibatch_size = int(batch_size // params[\"num_minibatches\"])\n",
    "num_iterations = params[\"total_timesteps\"] // batch_size     \n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "next_obs = torch.Tensor(envs.reset()[0]).to(device)\n",
    "next_done = torch.zeros(params[\"num_envs\"]).to(device)\n",
    "results_simple_PPO = {\"global_step\":[],\n",
    "                      \"return_value\":[]}\n",
    "\n",
    "tracking_global_step = 0\n",
    "\n",
    "for iteration in range(1, num_iterations+1):\n",
    "    if params[\"anneal_lr\"]:\n",
    "        updated_lr = (1.0 - (iteration - 1.0) / num_iterations) * params[\"learning_rate\"]\n",
    "        optimizer.param_groups[0][\"lr\"] = updated_lr\n",
    "    \n",
    "    for step in range(0, params[\"num_steps\"]):\n",
    "        global_step += 1 * params[\"num_envs\"]\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done \n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = Agent.get_action_and_value(next_obs)\n",
    "        values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "        \n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "        done = np.logical_or(terminated, truncated)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "        if \"final_info\" in info:\n",
    "            for info in info[\"final_info\"]:\n",
    "                if info and \"episode\" in info:\n",
    "                    print(\n",
    "                    f\"global_step={global_step}, episodic_return={info['episode']['r'][0]}\"\n",
    "                        )\n",
    "                    \n",
    "        if global_step - tracking_global_step > 2000:\n",
    "            return_eval = evaluate(params[\"env_id\"], Agent, use_int_rews=False)\n",
    "            results_simple_PPO[\"global_step\"].append(global_step)\n",
    "            results_simple_PPO[\"return_value\"].append(return_eval)\n",
    "            tracking_global_step = global_step\n",
    "\n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value = Agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(params[\"num_steps\"])):\n",
    "            if t == params[\"num_steps\"] - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            delta = rewards[t] + params[\"gamma\"] * nextvalues * nextnonterminal - values[t]\n",
    "            advantages[t] = lastgaelam = delta + params[\"gamma\"] * params[\"gae_lambda\"] * nextnonterminal * lastgaelam\n",
    "        returns = advantages + values\n",
    "\n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(params[\"update_epochs\"]):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = Agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > params[\"clip_coef\"]).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if params[\"norm_adv\"]:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - params[\"clip_coef\"], 1 + params[\"clip_coef\"])\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            newvalue = newvalue.view(-1)\n",
    "            if params[\"clip_vloss\"]:\n",
    "                value_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                    newvalue - b_values[mb_inds],\n",
    "                    -params[\"clip_coef\"],\n",
    "                    params[\"clip_coef\"],\n",
    "                )\n",
    "                value_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                value_loss_max = torch.max(value_loss_unclipped, value_loss_clipped)\n",
    "                value_loss = 0.5 * value_loss_max.mean()\n",
    "            else:\n",
    "                value_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - params[\"ent_coef\"] * entropy_loss + value_loss * params[\"vf_coef\"]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(Agent.parameters(), params[\"max_grad_norm\"])\n",
    "            optimizer.step()\n",
    "\n",
    "        if params[\"target_kl\"] is not None and approx_kl > params[\"target_kl\"]:\n",
    "            break\n",
    "\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(Agent, \"pretrained_models/simple_ppo_for_suprised.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from results_simple_PPO\n",
    "ppo_global_step = results_simple_PPO[\"global_step\"]\n",
    "ppo_return_value = results_simple_PPO[\"return_value\"]\n",
    "\n",
    "\n",
    "df_ppo = pd.DataFrame({'global_step': ppo_global_step, 'return_value': ppo_return_value})\n",
    "\n",
    "\n",
    "df_ppo.to_csv('./data/results_simple_ppo.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Random Network Distillation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Key Points**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prediction problem is randomly generated. This involves 2 NNs, fixed target network sets the prediction problem (find an embedding $f(O)$ for an observation) and predictor network trained on data collected (with the task to predict $\\hat{f}(O)$) from the agent, minimizing MSE Loss $\\text{MSE} = \\| \\hat{f}(x; \\theta) - f(x) \\|^{2}_2$.\n",
    "- Prediction error is expected to be higher in novel state (suprise state) that the agent is not familiar with. \n",
    "- $R=R_E+R_I$, thus, $V=V_E+V_I$.\n",
    "- Reward and Observation Normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.1 RND Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNDModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Prediction network\n",
    "        self.predictor = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_space, 256)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(256, 256)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(256, 256), std=1.0),\n",
    "        )\n",
    "\n",
    "        # Target network\n",
    "        self.target = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_space, 256)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(256, 256)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(256, 256), std=1.0),)\n",
    "\n",
    "        # fixed the target network params\n",
    "        for param in self.target.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, next_obs):\n",
    "        target_feature = self.target(next_obs)\n",
    "        predict_feature = self.predictor(next_obs)\n",
    "\n",
    "        return predict_feature, target_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingSumOfReward:\n",
    "    def __init__(self, gamma):\n",
    "        self.moving_sum_of_reward = None\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, rews):\n",
    "        if self.moving_sum_of_reward is None:\n",
    "            self.moving_sum_of_reward = rews\n",
    "        else:\n",
    "            self.moving_sum_of_reward = self.moving_sum_of_reward * self.gamma + rews\n",
    "        return self.moving_sum_of_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.2.2 Main Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = params[\"env_id\"]\n",
    "exp_name = params[\"exp_name\"]\n",
    "seed = params[\"seed\"]\n",
    "run_name = f\"{env_id}__{exp_name}__{seed}__{int(time.time())}\"\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id) for i in range(params[\"num_envs\"])],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up agent and model\n",
    "Agent = PPOAgent(envs, use_int_rews=True).to(device)\n",
    "optimizer = optim.Adam(\n",
    "    Agent.parameters(),\n",
    "    lr=params[\"learning_rate\"],\n",
    "    eps=1e-5,\n",
    ")\n",
    "rnd_model = RNDModel().to(device)\n",
    "combined_parameters = list(Agent.parameters()) + list(rnd_model.predictor.parameters())\n",
    "optimizer = optim.Adam(\n",
    "    combined_parameters,\n",
    "    lr=params[\"learning_rate\"],\n",
    "    eps=1e-5,\n",
    ")\n",
    "\n",
    "rew_runnning_mean_std = RunningMeanStd()\n",
    "discounted_reward = MovingSumOfReward(params[\"int_gamma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_observation_space.shape).to(device)  # (128, 4,, 4, 84, 84)\n",
    "actions = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_action_space.shape).to(device)   # (128, 4) \n",
    "logprobs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "surprise_rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "dones = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "ext_values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "int_values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "avg_returns = deque(maxlen=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(params[\"num_envs\"] * params[\"num_steps\"])                      # 4 * 128\n",
    "minibatch_size = int(batch_size // params[\"num_minibatches\"])\n",
    "num_iterations = params[\"total_timesteps\"] // batch_size                        # 20000000/(4*128) -> num iterations\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "next_obs = torch.Tensor(envs.reset()[0]).to(device)\n",
    "next_done = torch.zeros(params[\"num_envs\"]).to(device)\n",
    "\n",
    "results_RND = {\"global_step\":[],\n",
    "                \"return_value\":[],\n",
    "                \"intrinsic_reward\":[]}\n",
    "\n",
    "tracking_global_step = 0\n",
    "\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    if params[\"anneal_lr\"]:\n",
    "        updated_lr = (1.0 - (iteration - 1.0) / num_iterations) * params[\"learning_rate\"]\n",
    "        optimizer.param_groups[0][\"lr\"] = updated_lr\n",
    "\n",
    "    # n-step rollouts\n",
    "    for step in range(0, params[\"num_steps\"]):\n",
    "        global_step += 1 * params[\"num_envs\"]\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value_ext, value_int = Agent.get_action_and_value(next_obs)\n",
    "        \n",
    "        ext_values[step], int_values[step] = (\n",
    "                value_ext.flatten(),\n",
    "                value_int.flatten(),\n",
    "            )\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob.flatten()\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "        done = np.logical_or(terminated, truncated)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "        # Normalize obs for rnd\n",
    "        rnd_next_obs = next_obs\n",
    "        \n",
    "        # Get the target F(O) and predict \\hat(F)(O) value from rnd model\n",
    "        target_next_feature, predict_next_feature = rnd_model.target(rnd_next_obs), rnd_model.predictor(rnd_next_obs)\n",
    "\n",
    "        # Calculate the surprise reward\n",
    "        surprise_rewards[step] = ((target_next_feature - predict_next_feature).pow(2).sum(1) / 2).data\n",
    "\n",
    "        if \"final_info\" in info:\n",
    "            for info in info[\"final_info\"]:\n",
    "                if info and \"episode\" in info:\n",
    "                    print(\n",
    "                    f\"global_step={global_step}, episodic_return={info['episode']['r'][0]}, surprise_reward={np.mean(surprise_rewards[step].cpu().numpy())}\"\n",
    "                        )\n",
    "                    \n",
    "        if global_step - tracking_global_step > 2000:\n",
    "                return_eval = evaluate(params[\"env_id\"], Agent, use_int_rews=True)\n",
    "                results_RND[\"global_step\"].append(global_step)\n",
    "                results_RND[\"return_value\"].append(return_eval)\n",
    "                tracking_global_step = global_step\n",
    "\n",
    "\n",
    "    # Calculate the discounted reward \n",
    "    surprise_reward_per_env = np.array(\n",
    "        [discounted_reward.update(reward_per_step) for reward_per_step in surprise_rewards.cpu().data.numpy().T]\n",
    "    )\n",
    "\n",
    "    mean, std, count = (\n",
    "        np.mean(surprise_reward_per_env),\n",
    "        np.std(surprise_reward_per_env),\n",
    "        len(surprise_reward_per_env),\n",
    "    )\n",
    "    \n",
    "    rew_runnning_mean_std.update_from_moments(mean, std**2, count)\n",
    "\n",
    "    # Normalize the curiousity_rewards based on the running_mean_std\n",
    "    surprise_rewards /= np.sqrt(rew_runnning_mean_std.var)\n",
    "\n",
    "    # Calculate value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value_ext, next_value_int = Agent.get_value(next_obs)\n",
    "        next_value_ext, next_value_int = next_value_ext.reshape(1, -1), next_value_int.reshape(1, -1)   # -> get next state values external & internal\n",
    "        ext_advantages = torch.zeros_like(rewards, device=device)\n",
    "        int_advantages = torch.zeros_like(surprise_rewards, device=device)\n",
    "        ext_lastgaelam = 0\n",
    "        int_lastgaelam = 0\n",
    "        for t in reversed(range(params[\"num_steps\"])):\n",
    "            if t == params[\"num_steps\"] - 1:\n",
    "                ext_nextnonterminal = 1.0 - next_done\n",
    "                int_nextnonterminal = 1.0\n",
    "                ext_nextvalues = next_value_ext\n",
    "                int_nextvalues = next_value_int\n",
    "            else:\n",
    "                ext_nextnonterminal = 1.0 - dones[t + 1]\n",
    "                int_nextnonterminal = 1.0\n",
    "                ext_nextvalues = ext_values[t + 1]\n",
    "                int_nextvalues = int_values[t + 1]\n",
    "            ext_delta = rewards[t] + params[\"gamma\"] * ext_nextvalues * ext_nextnonterminal - ext_values[t]\n",
    "            int_delta = surprise_rewards[t] + params[\"int_gamma\"] * int_nextvalues * int_nextnonterminal - int_values[t]\n",
    "            ext_advantages[t] = ext_lastgaelam = (\n",
    "                ext_delta + params[\"gamma\"] * params[\"gae_lambda\"] * ext_nextnonterminal * ext_lastgaelam\n",
    "            )\n",
    "            int_advantages[t] = int_lastgaelam = (\n",
    "                int_delta + params[\"int_gamma\"] * params[\"gae_lambda\"] * int_nextnonterminal * int_lastgaelam\n",
    "            )\n",
    "        ext_returns = ext_advantages + ext_values\n",
    "        int_returns = int_advantages + int_values\n",
    "\n",
    "    # Collect batch data for optimization\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape(-1)\n",
    "    b_ext_advantages = ext_advantages.reshape(-1)\n",
    "    b_int_advantages = int_advantages.reshape(-1)\n",
    "    b_ext_returns = ext_returns.reshape(-1)\n",
    "    b_int_returns = int_returns.reshape(-1)\n",
    "    b_ext_values = ext_values.reshape(-1)\n",
    "\n",
    "    b_advantages = b_int_advantages * params[\"int_coef\"] + b_ext_advantages * params[\"ext_coef\"]\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(batch_size)\n",
    "\n",
    "    rnd_next_obs = b_obs\n",
    "\n",
    "    clipfracs = []\n",
    "    for epoch in range(params[\"update_epochs\"]):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            # Forward_loss\n",
    "            predict_next_state_feature, target_next_state_feature = rnd_model(rnd_next_obs[mb_inds])\n",
    "            forward_loss = F.mse_loss(\n",
    "                predict_next_state_feature, target_next_state_feature.detach(), reduction=\"none\"\n",
    "            ).mean(-1)\n",
    "\n",
    "            mask = torch.rand(len(forward_loss), device=device)\n",
    "            mask = (mask < params[\"update_proportion\"]).type(torch.FloatTensor).to(device)\n",
    "            forward_loss = (forward_loss * mask).sum() / torch.max(\n",
    "                mask.sum(), torch.tensor([1], device=device, dtype=torch.float32)\n",
    "            )\n",
    "            _, newlogprob, entropy, new_ext_values, new_int_values = Agent.get_action_and_value(\n",
    "                b_obs[mb_inds], b_actions.long()[mb_inds]\n",
    "            )\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                old_approx_kl = (-logratio).mean()\n",
    "                approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                clipfracs += [((ratio - 1.0).abs() > params[\"clip_coef\"]).float().mean().item()]\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if params[\"norm_adv\"]:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - params[\"clip_coef\"], 1 + params[\"clip_coef\"])\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            new_ext_values, new_int_values = new_ext_values.view(-1), new_int_values.view(-1)\n",
    "            if params[\"clip_vloss\"]:\n",
    "                ext_value_loss_unclipped = (new_ext_values - b_ext_returns[mb_inds]) ** 2\n",
    "                ext_v_clipped = b_ext_values[mb_inds] + torch.clamp(\n",
    "                    new_ext_values - b_ext_values[mb_inds],\n",
    "                    -params[\"clip_coef\"],\n",
    "                params[\"clip_coef\"],\n",
    "                )\n",
    "                ext_value_loss_clipped = (ext_v_clipped - b_ext_returns[mb_inds]) ** 2\n",
    "                ext_value_loss_max = torch.max(ext_value_loss_unclipped, ext_value_loss_clipped)\n",
    "                ext_value_loss = 0.5 * ext_value_loss_max.mean()\n",
    "            else:\n",
    "                ext_value_loss = 0.5 * ((new_ext_values - b_ext_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            int_value_loss = 0.5 * ((new_int_values - b_int_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            value_loss = ext_value_loss + int_value_loss\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - params[\"ent_coef\"] * entropy_loss + value_loss * params[\"vf_coef\"] + forward_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if params[\"max_grad_norm\"]:\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    combined_parameters,\n",
    "                    params[\"max_grad_norm\"],\n",
    "                )\n",
    "            optimizer.step()\n",
    "\n",
    "        if params[\"target_kl\"] is not None:\n",
    "            if approx_kl > params[\"target_kl\"]:\n",
    "                break\n",
    "\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(Agent, \"pretrained_models/ppo_for_RND.pth\")\n",
    "# torch.save(rnd_model, \"pretrained_models/rnd.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from results_RND\n",
    "rnd_global_step = results_RND[\"global_step\"]\n",
    "rnd_return_value = results_RND[\"return_value\"]\n",
    "\n",
    "df_rnd = pd.DataFrame({'global_step': rnd_global_step, 'return_value': rnd_return_value})\n",
    "\n",
    "# Save DataFrames to CSV files\n",
    "df_rnd.to_csv('data/results_rnd.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Curiosity-driven Exploration by Self-supervised Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1 Key Points**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The architecture is a network with two tasks, which we refer to as inverse prediction task and forward prediction task. The network, firstly, encodes the state $s_{t}$ and state $s_{t+1}$ into feature vectors $\\phi(s_{t})$ and $\\phi(s_{t+1})$. The network used the two encoded vectors as input to predict action $a_t$ in the inverse prediction task. It then used the result feature vectors $\\phi(s_{t})$ and action $a_t$ as input to predict $\\phi(s_{t+1})$. \n",
    "- The loss for the inverse prediction task is a cross entropy loss between the action chosen by the architecture and the real action that the agent has taken. The loss for the forward prediction task is an MSE loss.\n",
    "- Prediction error is expected to be higher in novel state (suprise state) that the agent is not familiar with. \n",
    "- Reward and Observation Normalization.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./image/Curiousity-driven exploration.png\" alt=\"AutoEncoder forr Count Based Exploration\" width=\"420\" height=\"350\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2 Models**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.2.1 ICM Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICMModel(nn.Module):\n",
    "    def __init__(self, use_cuda=True):\n",
    "        super(ICMModel, self).__init__()\n",
    "\n",
    "        self.eta = 1.\n",
    "        self.device = device\n",
    "\n",
    "        self.feature = nn.Sequential(\n",
    "            layer_init(nn.Linear(state_space, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.ReLU(),\n",
    "            layer_init(nn.Linear(64, 64), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.inverse_net = nn.Sequential(\n",
    "            nn.Linear(64 * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_space)\n",
    "        )\n",
    "\n",
    "        self.residual = [nn.Sequential(\n",
    "            nn.Linear(action_space + 512, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "        ).to(self.device)] * 4\n",
    "\n",
    "        self.forward_net_1 = nn.Sequential(\n",
    "            nn.Linear(action_space + 64, 512),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.forward_net_2 = nn.Sequential(\n",
    "            nn.Linear(action_space + 512, 64),\n",
    "        )\n",
    "\n",
    "        for p in self.modules():\n",
    "            if isinstance(p, nn.Linear):\n",
    "                init.kaiming_uniform_(p.weight, a=1.0)\n",
    "                p.bias.data.zero_()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        state, next_state, action = inputs\n",
    "\n",
    "        encode_state = self.feature(state)\n",
    "        encode_next_state = self.feature(next_state)\n",
    "        # get pred action\n",
    "        pred_action = torch.cat((encode_state, encode_next_state), 1)\n",
    "        pred_action = self.inverse_net(pred_action)\n",
    "        # ---------------------\n",
    "\n",
    "        # get pred next state\n",
    "        pred_next_state_feature_orig = torch.cat((encode_state, action), 1)\n",
    "        pred_next_state_feature_orig = self.forward_net_1(pred_next_state_feature_orig)\n",
    "\n",
    "        # residual\n",
    "        for i in range(2):\n",
    "            pred_next_state_feature = self.residual[i * 2](torch.cat((pred_next_state_feature_orig, action), 1))\n",
    "            pred_next_state_feature_orig = self.residual[i * 2 + 1](\n",
    "                torch.cat((pred_next_state_feature, action), 1)) + pred_next_state_feature_orig\n",
    "\n",
    "        pred_next_state_feature = self.forward_net_2(torch.cat((pred_next_state_feature_orig, action), 1))\n",
    "\n",
    "        real_next_state_feature = encode_next_state\n",
    "        return real_next_state_feature, pred_next_state_feature, pred_action\n",
    "    \n",
    "    def compute_intrinsic_reward(self, state, next_state, action):\n",
    "        # Create 0 vector for the onehot encoded action\n",
    "        action_onehot = torch.zeros(len(action), action_space, device=self.device)\n",
    "        # Scatter the value as according to the value in action\n",
    "        action_onehot.scatter_(1, action.view(len(action), -1), 1)\n",
    "\n",
    "        real_next_state_feature, pred_next_state_feature, pred_action = self.forward([state, next_state, action_onehot])\n",
    "        intrinsic_reward = self.eta * F.mse_loss(real_next_state_feature, pred_next_state_feature, reduction='none').mean(-1)\n",
    "        return intrinsic_reward\n",
    "    \n",
    "    def inference(self, states, next_states, actions):\n",
    "        action_onehot = torch.zeros(len(actions), action_space, device=self.device)\n",
    "        action_onehot.scatter_(1, actions.view(-1, 1).long(), 1)\n",
    "\n",
    "        real_next_state_feature, pred_next_state_feature, pred_action = self.forward([states, next_states, action_onehot])\n",
    "        return real_next_state_feature, pred_next_state_feature, pred_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingSumOfReward:\n",
    "    def __init__(self, gamma):\n",
    "        self.moving_sum_of_reward = None\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, rews):\n",
    "        if self.moving_sum_of_reward is None:\n",
    "            self.moving_sum_of_reward = rews\n",
    "        else:\n",
    "            self.moving_sum_of_reward = self.moving_sum_of_reward * self.gamma + rews\n",
    "        return self.moving_sum_of_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.2.2 Main Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = params[\"env_id\"]\n",
    "exp_name = params[\"exp_name\"]\n",
    "seed = params[\"seed\"]\n",
    "run_name = f\"{env_id}__{exp_name}__{seed}__{int(time.time())}\"\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv(\n",
    "    [make_env(env_id) for i in range(params[\"num_envs\"])],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up agent and model\n",
    "icm = ICMModel().to(device)\n",
    "Agent = PPOAgent(envs, use_int_rews=True).to(device)\n",
    "\n",
    "combined_parameters = list(Agent.parameters() ) + list(icm.parameters())\n",
    "optimizer = optim.Adam(\n",
    "    combined_parameters,\n",
    "    lr=params[\"learning_rate\"],\n",
    "    eps=1e-5,\n",
    ")\n",
    "\n",
    "rew_runnning_mean_std = RunningMeanStd()\n",
    "discounted_reward = MovingSumOfReward(params[\"int_gamma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((params[\"num_steps\"]+1, params[\"num_envs\"]) + envs.single_observation_space.shape).to(device)  # (128, 4,, 4, 84, 84)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(params[\"num_envs\"] * params[\"num_steps\"])                      # 4 * 128\n",
    "minibatch_size = int(batch_size // params[\"num_minibatches\"])\n",
    "num_iterations = params[\"total_timesteps\"] // batch_size                        # 20000000/(4*128) -> num iterations\n",
    "global_step = 0\n",
    "tracking_global_step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "next_obs = torch.Tensor(envs.reset()[0]).to(device)\n",
    "next_done = torch.zeros(params[\"num_envs\"]).to(device)\n",
    "\n",
    "results_ICM = {\"global_step\":[],\n",
    "                \"return_value\":[],\n",
    "                \"intrinsic_reward\":[]}\n",
    "\n",
    "for iteration in range(1, num_iterations + 1):\n",
    "    actions = torch.zeros((params[\"num_steps\"], params[\"num_envs\"]) + envs.single_action_space.shape).to(device)   \n",
    "    logprobs = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    surprise_rewards = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    dones = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    ext_values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "    int_values = torch.zeros((params[\"num_steps\"], params[\"num_envs\"])).to(device)\n",
    "\n",
    "    # Calculate the new learning rate as according to the annealing rate if needed.\n",
    "    if params[\"anneal_lr\"]:\n",
    "        updated_lr = (1.0 - (iteration - 1.0) / num_iterations) * params[\"learning_rate\"]\n",
    "        optimizer.param_groups[0][\"lr\"] = updated_lr\n",
    "\n",
    "    for step in range(0, params[\"num_steps\"]):\n",
    "        global_step += 1 * params[\"num_envs\"]\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value_ext, value_int = Agent.get_action_and_value(obs[step])\n",
    "        \n",
    "        ext_values[step], int_values[step] = (\n",
    "                value_ext.flatten(),\n",
    "                value_int.flatten(),\n",
    "            )       \n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob.flatten()        \n",
    "        next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
    "        done = np.logical_or(terminated, truncated)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "        # next_obs_b[step] = next_obs\n",
    "\n",
    "        icm_obs = obs[step]\n",
    "        icm_next_obs = next_obs\n",
    "        \n",
    "        surprise_rewards[step] = icm.compute_intrinsic_reward(icm_obs.to(device), icm_next_obs.to(device), actions[step].long())\n",
    "        \n",
    "        if \"final_info\" in info:\n",
    "            for info in info[\"final_info\"]:\n",
    "                if info and \"episode\" in info:\n",
    "                    print(\n",
    "                    f\"global_step={global_step}, episodic_return={info['episode']['r'][0]}, surprise_reward={np.mean(surprise_rewards[step].data.cpu().numpy())}\"\n",
    "                        )\n",
    "                    \n",
    "        if global_step - tracking_global_step > 2000:\n",
    "                return_eval = evaluate(params[\"env_id\"], Agent, use_int_rews=True)\n",
    "                results_ICM[\"global_step\"].append(global_step)\n",
    "                results_ICM[\"return_value\"].append(return_eval)\n",
    "                tracking_global_step = global_step\n",
    "            \n",
    "    obs[-1] = next_obs\n",
    "\n",
    "    # Calculate the discounted reward \n",
    "    surprise_reward_per_env = np.array(\n",
    "        [discounted_reward.update(reward_per_step) for reward_per_step in surprise_rewards.cpu().data.numpy().T]\n",
    "    )\n",
    "\n",
    "    mean, std, count = (\n",
    "        np.mean(surprise_reward_per_env),\n",
    "        np.std(surprise_reward_per_env),\n",
    "        len(surprise_reward_per_env),\n",
    "    )\n",
    "    \n",
    "    rew_runnning_mean_std.update_from_moments(mean, std**2, count)\n",
    "\n",
    "    # Normalize the curiousity_rewards based on the running_mean_std\n",
    "    surprise_rewards /= np.sqrt(rew_runnning_mean_std.var)\n",
    "\n",
    "    # Calculate value if not done\n",
    "    with torch.no_grad():\n",
    "        next_value_ext, next_value_int = Agent.get_value(next_obs)\n",
    "        next_value_ext, next_value_int = next_value_ext.reshape(1, -1), next_value_int.reshape(1, -1)   \n",
    "        ext_advantages = torch.zeros_like(rewards, device=device)\n",
    "        int_advantages = torch.zeros_like(surprise_rewards, device=device)\n",
    "        ext_lastgaelam = 0\n",
    "        int_lastgaelam = 0\n",
    "        for t in reversed(range(params[\"num_steps\"])):\n",
    "            if t == params[\"num_steps\"] - 1:\n",
    "                ext_nextnonterminal = 1.0 - next_done\n",
    "                int_nextnonterminal = 1.0\n",
    "                ext_nextvalues = next_value_ext\n",
    "                int_nextvalues = next_value_int\n",
    "            else:\n",
    "                ext_nextnonterminal = 1.0 - dones[t + 1]\n",
    "                int_nextnonterminal = 1.0\n",
    "                ext_nextvalues = ext_values[t + 1]\n",
    "                int_nextvalues = int_values[t + 1]\n",
    "            ext_delta = rewards[t] + params[\"gamma\"] * ext_nextvalues * ext_nextnonterminal - ext_values[t]\n",
    "            int_delta = surprise_rewards[t] + params[\"int_gamma\"] * int_nextvalues * int_nextnonterminal - int_values[t]\n",
    "            ext_advantages[t] = ext_lastgaelam = (\n",
    "                ext_delta + params[\"gamma\"] * params[\"gae_lambda\"] * ext_nextnonterminal * ext_lastgaelam\n",
    "            )\n",
    "            int_advantages[t] = int_lastgaelam = (\n",
    "                int_delta + params[\"int_gamma\"] * params[\"gae_lambda\"] * int_nextnonterminal * int_lastgaelam\n",
    "            )\n",
    "        ext_returns = ext_advantages + ext_values\n",
    "        int_returns = int_advantages + int_values\n",
    "    \n",
    "    # Collect batch data for optimization\n",
    "    b_obs = obs[:-1].reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_next_obs = obs[1:].reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape(-1)\n",
    "    b_ext_advantages = ext_advantages.reshape(-1)\n",
    "    b_int_advantages = int_advantages.reshape(-1)\n",
    "    b_ext_returns = ext_returns.reshape(-1)\n",
    "    b_int_returns = int_returns.reshape(-1)\n",
    "    b_ext_values = ext_values.reshape(-1)\n",
    "\n",
    "    b_advantages = b_int_advantages * params[\"int_coef\"] + b_ext_advantages * params[\"ext_coef\"]\n",
    "\n",
    "    # Optimizing the policy and value network\n",
    "    b_inds = np.arange(batch_size)\n",
    "    icm_obs = b_obs\n",
    "    icm_next_obs = b_next_obs\n",
    "\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    forward_mse = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(params[\"update_epochs\"]):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            real_next_state_feature, pred_next_state_feature, pred_action = icm.inference(icm_obs[mb_inds].to(device), icm_next_obs[mb_inds].to(device), b_actions[mb_inds])\n",
    "\n",
    "            inverse_loss = ce(\n",
    "                    pred_action, b_actions[mb_inds].long())\n",
    "\n",
    "            forward_loss = forward_mse(\n",
    "                    pred_next_state_feature, real_next_state_feature.detach())\n",
    "\n",
    "            _, newlogprob, entropy, new_ext_values, new_int_values = Agent.get_action_and_value(\n",
    "                b_obs[mb_inds], b_actions.long()[mb_inds]\n",
    "            )\n",
    "            \n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "            if params[\"norm_adv\"]:\n",
    "                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - params[\"clip_coef\"], 1 + params[\"clip_coef\"])\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            new_ext_values, new_int_values = new_ext_values.view(-1), new_int_values.view(-1)\n",
    "            if params[\"clip_vloss\"]:\n",
    "                ext_value_loss_unclipped = (new_ext_values - b_ext_returns[mb_inds]) ** 2\n",
    "                ext_v_clipped = b_ext_values[mb_inds] + torch.clamp(\n",
    "                    new_ext_values - b_ext_values[mb_inds],\n",
    "                    -params[\"clip_coef\"],\n",
    "                params[\"clip_coef\"],\n",
    "                )\n",
    "                ext_value_loss_clipped = (ext_v_clipped - b_ext_returns[mb_inds]) ** 2\n",
    "                ext_value_loss_max = torch.max(ext_value_loss_unclipped, ext_value_loss_clipped)\n",
    "                ext_value_loss = 0.5 * ext_value_loss_max.mean()\n",
    "            else:\n",
    "                ext_value_loss = 0.5 * ((new_ext_values - b_ext_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            int_value_loss = 0.5 * ((new_int_values - b_int_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            value_loss = ext_value_loss + int_value_loss\n",
    "            entropy_loss = entropy.mean()\n",
    "\n",
    "            loss = pg_loss - params[\"ent_coef\"] * entropy_loss + value_loss * params[\"vf_coef\"] + forward_loss + inverse_loss\n",
    "        \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            if params[\"max_grad_norm\"]:\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    combined_parameters,\n",
    "                    params[\"max_grad_norm\"],\n",
    "                )\n",
    "            optimizer.step()\n",
    "\n",
    "    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(Agent, \"pretrained_models/ppo_for_ICM.pth\")\n",
    "# torch.save(icm, \"pretrained_models/icm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from results_RND\n",
    "icm_global_step = results_ICM[\"global_step\"]\n",
    "icm_return_value = results_ICM[\"return_value\"]\n",
    "icm_intrinsic_reward = results_ICM[\"intrinsic_reward\"]\n",
    "\n",
    "df_icm = pd.DataFrame({'global_step': icm_global_step, 'return_value': icm_return_value})\n",
    "\n",
    "# Save DataFrames to CSV files\n",
    "df_icm.to_csv('data/results_icm.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Results Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simple_PPO = pd.read_csv('data/results_simple_ppo.csv')\n",
    "df_rnd = pd.read_csv('data/results_rnd.csv')\n",
    "df_icm = pd.read_csv('data/results_icm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_simple_PPO, df_rnd, df_icm]\n",
    "\n",
    "for df in dfs:\n",
    "    df[\"return_value_smoothed\"] = df[\"return_value\"].ewm(alpha=1-0.9).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data\n",
    "plt.plot(df_simple_PPO[\"global_step\"], df_simple_PPO[\"return_value_smoothed\"], label='Simple PPO')\n",
    "plt.plot(df_rnd[\"global_step\"], df_rnd[\"return_value_smoothed\"], label='RND')\n",
    "plt.plot(df_icm[\"global_step\"], df_icm[\"return_value_smoothed\"], label='ICM')\n",
    "plt.xlabel('Global Step')\n",
    "plt.ylabel('Return Value')\n",
    "plt.title('Return of 1 run  (Evaluate over 50 episodes)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
